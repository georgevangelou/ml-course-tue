{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0pKg2srKaejL"
   },
   "outputs": [],
   "source": [
    "# Fill in your name using the given format\n",
    "your_name = \"EVANGELOU, GEORGIOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4j0-kHyM_5m4",
    "outputId": "24747993-43d8-4f8b-93e7-005ec58f26bb"
   },
   "outputs": [],
   "source": [
    "# For use in colab\n",
    "from IPython import get_ipython\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install openml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbPssiO9aejN"
   },
   "source": [
    "# Assignment 1\n",
    "We will study classifiers on the [Kuzushiji dataset](https://www.openml.org/d/41982).\n",
    "As you can [read in the paper](https://arxiv.org/abs/1812.01718), Kuzushiji is a handwritten \n",
    "Japanese script that was used for thousands of years, but most Japanese natives cannot read\n",
    "it anymore. We'll try to use machine learning to recognize the characters, and translate them to\n",
    "modern Japanese (Hiragana) characters.\n",
    "\n",
    "The dataset that we will use contains scanned 28-by-28 pixel images of such handwritten characters. Actually, only 10 of those characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcLHcftVaejP",
    "outputId": "9319122b-096a-4904-e4ed-303c7de9458a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. You may continue :)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openml\n",
    "import time\n",
    "import math\n",
    "import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# Pre-flight checklist. Do not change this code.\n",
    "# Make sure that you have installed recent versions of key packages.\n",
    "from packaging import version\n",
    "import sklearn\n",
    "sklearn_version = sklearn.__version__\n",
    "if version.parse(sklearn_version) < version.parse(\"1.0.2\"):\n",
    "    print(\"scikit-learn is outdated: {}. Please update now! pip install -U scikit-learn\".format(sklearn_version))\n",
    "else:\n",
    "    print(\"OK. You may continue :)\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QA-17nataejQ"
   },
   "outputs": [],
   "source": [
    "# Download Kuzushiji-MNIST data. Takes a while the first time.\n",
    "# You may receive a warning about data compression but you can ignore that.\n",
    "# Don't overwrite the X and y variables anywhere in this notebook. \n",
    "# Seriously, don't.\n",
    "data = openml.datasets.get_dataset(41982)\n",
    "X, y, _, _ = data.get_data(target=data.default_target_attribute);\n",
    "X = X/255 # Simple scaling \n",
    "\n",
    "# These are the names of the modern characters (our 10 classes)\n",
    "data_classes = {0:\"o\", 1: \"ki\", 2: \"su\", 3: \"tsu\", 4: \"na\", 5: \"ha\", \n",
    "                6: \"ma\", 7: \"ya\", 8: \"re\", 9: \"wo\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cZRw5I6WaejR"
   },
   "outputs": [],
   "source": [
    "# Plotting helper functions. Don't edit these.\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def plot_live(X, y, evaluator, param_name, param_range, scale='log', ylim=(0,1), ylabel='score', marker = '.'):\n",
    "    \"\"\" Renders a plot that updates with every evaluation from the evaluator.\n",
    "    Keyword arguments:\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    evaluator -- a function with signature (X, y, param_value) that returns a dictionary of scores.\n",
    "                 Examples: {\"train\": 0.9, \"test\": 0.95} or {\"model_1\": 0.9, \"model_2\": 0.7}\n",
    "    param_name -- the parameter that is being varied on the X axis. Can be a hyperparameter, sample size,...\n",
    "    param_range -- list of all possible values on the x-axis\n",
    "    scale -- defines which scale to plot the x-axis on, either 'log' (logarithmic) or 'linear'\n",
    "    ylim -- tuple with the lowest and highest y-value to plot (e.g. (0, 1))\n",
    "    ylabel -- the y-axis title\n",
    "    \"\"\"\n",
    "    # Plot interactively\n",
    "    plt.ion()\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(param_name)\n",
    "    \n",
    "    # Make the scale look nice\n",
    "    plt.xscale(scale)\n",
    "    plt.xlim(param_range[0],param_range[-1])\n",
    "    plt.ylim(ylim)\n",
    "        \n",
    "    # Start from empty plot, then fill it\n",
    "    series = {}\n",
    "    lines = {}\n",
    "    xvals = []\n",
    "    for i in param_range:\n",
    "        scores = evaluator(X, y, i) \n",
    "        if i == param_range[0]: # initialize series\n",
    "            for k in scores.keys():\n",
    "                lines[k], = plt.plot(xvals, [], marker = marker, label = k)\n",
    "                series[k] = []\n",
    "        xvals.append(i)\n",
    "        for k in scores.keys(): # append new data\n",
    "            series[k].append(scores[k])\n",
    "            lines[k].set_data(xvals, series[k])\n",
    "        # refresh plot\n",
    "        plt.legend(loc='best')\n",
    "        plt.margins(0.1)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "# Plots data instances as 28x28 images\n",
    "def plot_examples(images, labels, row_length=5, title=\"\"):\n",
    "    \"\"\" Renders a set of images\n",
    "    images -- an array of 28x28 images as 1D vectors\n",
    "    labels -- the corresponding labels for each image\n",
    "    row_length -- how many images should be shown per line\n",
    "    title -- a title for the produced figure\n",
    "    \"\"\"\n",
    "    nr_rows = math.floor(len(images) / row_length)\n",
    "    if (len(images) % row_length) > 0:\n",
    "        nr_rows += 1\n",
    "    fig, axes = plt.subplots(nr_rows, row_length, figsize=(1.5 * row_length, 1.5 * nr_rows))\n",
    "    for i, n in enumerate(images):\n",
    "        if len(images) > row_length:\n",
    "            axes[math.floor(i/row_length)][i%row_length].imshow(n.reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "            if labels is not None:\n",
    "              axes[math.floor(i/row_length)][i%row_length].set_xlabel(data_classes[int(labels[i])])\n",
    "            axes[math.floor(i/row_length)][i%row_length].set_xticks(())\n",
    "            axes[math.floor(i/row_length)][i%row_length].set_yticks(())\n",
    "\n",
    "        else:\n",
    "            axes[i].imshow(n.reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "            if labels is not None:\n",
    "              axes[i].set_xlabel(data_classes[int(labels[i])])\n",
    "            axes[i].set_xticks(())\n",
    "            axes[i].set_yticks(())\n",
    "    fig.suptitle(title);\n",
    "\n",
    "# Plots the coefficients of the given model as 28x28 heatmaps. \n",
    "def plot_coefficients(coef, name):\n",
    "    \"\"\" Renders a 28x28 heatmap of the model's trained coefficients.\n",
    "    Keyword arguments:\n",
    "    coef -- the model coefficients\n",
    "    name -- a title for the produced figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    fig.suptitle(name)\n",
    "    ax.imshow(coef.reshape(28,28))\n",
    "    ax.set_xticks(()), ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZgGoPwfaejS"
   },
   "source": [
    "### Peeking at the data\n",
    "If we plot the characters, we see that there is quite some variation. The same\n",
    "character can be written in a number of different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nJTbBZMUaejT",
    "outputId": "06f7f876-00a0-4749-c176-3f05adb76836"
   },
   "outputs": [],
   "source": [
    "# Gets indices of examples with the given class\n",
    "def y_class(c):\n",
    "    return y[y == str(c)].index.values.tolist()[10:20]\n",
    "\n",
    "for i in range(10):\n",
    "    break #REMOVE THIS\n",
    "    plot_examples(X.to_numpy()[y_class(i)], y.to_numpy()[y_class(i)], \n",
    "                  row_length=10, title=data_classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrp4EJcgFGpz"
   },
   "source": [
    "### Question 1.1: Cross-validate (1 point)\n",
    "Implement a method `evaluate_LR` that evaluates a Logistic Regression model for a given regularization constant (C) and returns the train and test score of a 5-fold stratified cross-validation using the accuracy metric. Note: we know that Logistic Regression is not the best technique for image data :). We'll use other techniques in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JTZptANpFWz6"
   },
   "outputs": [],
   "source": [
    "# Implement\n",
    "def evaluate_LR(X, y, C):\n",
    "    \"\"\" Evaluate logistic regression with 5-fold cross-validation on the provided (X, y) data. \n",
    "    Keyword arguments:\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    C -- the value for the regularization hyperparameter\n",
    "    \n",
    "    Returns: a dictionary with the mean train and test score, e.g. {\"train\": 0.9, \"test\": 0.95}\n",
    "    \"\"\"\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=250)\n",
    "    \n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # results = {}\n",
    "    # results[\"train\"] = model.score(X_train, y_train)\n",
    "    # results[\"test\"] = model.score(X_test, y_test)\n",
    "    \n",
    "    scores_temp = cross_validate(model, X, y, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    #print(scores_temp.keys())\n",
    "    results = {}\n",
    "    results[\"train\"] = scores_temp[\"train_score\"].mean()\n",
    "    results[\"test\"] = scores_temp[\"test_score\"].mean()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnoC4hfxaejV"
   },
   "source": [
    "### Question 1.2: Tune (1 points)\n",
    "\n",
    "Implement a method `plot_curve` that plots the results of `evaluate_LR` on a 25% stratified subsample of the Kuzushiji MNIST dataset for C values ranging from 1e-8 to 1e3 (on a log scale, at least 12 values). Use `random_state=0`. You can use the plotting function `plot_live` defined above (carefully read what it does), and add any helper functions you like. Note:  To be clear, you need to pass only 25% of the data to `evaluate_LR`. Using a 25% subsample won't give you optimal performance, but this is meant to make the assignment more doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEWF0sB8aejV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement. Do not change the name or signature of this function.\n",
    "def plot_curve(X,y):\n",
    "    \"\"\" Plots the train and test accuracy of logistic regression on a 25% \n",
    "    subsample of the given data for different amounts of regularization.\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    \n",
    "    Returns: a plot as described above, with C on the x-axis and accuracy on \n",
    "    the y-axis. \n",
    "    \"\"\"\n",
    "    \n",
    "    X_subsample = X[0 : int(len(X)/4)]\n",
    "    y_subsample = y[0 : int(len(X)/4)]\n",
    "    \n",
    "    C = [10**i for i in np.linspace(-8, 3, 12, endpoint=True)]\n",
    "    #print(C)\n",
    "    #C = [10**(i/10) for i in range(-60, 20, 1)]\n",
    "    plot_live(X_subsample, y_subsample, evaluate_LR, \"C\", C, \"log\", {0,1}, \"train\")\n",
    "    \"\"\" Renders a plot that updates with every evaluation from the evaluator.\n",
    "    Keyword arguments:\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    evaluator -- a function with signature (X, y, param_value) that returns a dictionary of scores.\n",
    "                 Examples: {\"train\": 0.9, \"test\": 0.95} or {\"model_1\": 0.9, \"model_2\": 0.7}\n",
    "    param_name -- the parameter that is being varied on the X axis. Can be a hyperparameter, sample size,...\n",
    "    param_range -- list of all possible values on the x-axis\n",
    "    scale -- defines which scale to plot the x-axis on, either 'log' (logarithmic) or 'linear'\n",
    "    ylim -- tuple with the lowest and highest y-value to plot (e.g. (0, 1))\n",
    "    ylabel -- the y-axis title\n",
    "    \"\"\"\n",
    "    \n",
    "plot_curve(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWCXN8RlUNAl"
   },
   "source": [
    "### Question 1.3: Interpretation (1 point)\n",
    "Interpret the graph. At C=1e-6, is the model underfitting, overfitting, or neither? What about the model trained with C=100? Below are a number of possible interpretations. Enter the correct letter in value `q_1.3` in the code.\n",
    "\n",
    "- 'A': Underfitting at C=1e-6, overfitting at C=100.  \n",
    "- 'B': Overfitting at C=1e-6, underfitting at C=100.  \n",
    "- 'C': Neither underfitting nor overfitting at C=1e-6, overfitting at C=100.\n",
    "- 'D': Neither underfitting nor overfitting at C=1e-6, underfitting at C=100.\n",
    "- 'E': Overfitting at C=1e-6, neither underfitting nor overfitting at C=100.\n",
    "- 'F': Underfitting at C=1e-6, neither underfitting nor overfitting at C=100.\n",
    "- 'G': Neither underfitting nor overfitting at both values for C.\n",
    "- 'H': No answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaQGRI_C7A10"
   },
   "outputs": [],
   "source": [
    "# Fill in the correct answer. Don't change the name of the variable\n",
    "q_1_3 = 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ris7uWnfdF0b"
   },
   "source": [
    "### Question 2.1: Model inspection (2 points)\n",
    "Implement a function `plot_tsu_coefficients` that plots a heatmap of the coefficients of three models (after training), one trained with C=1e-6, one with C=0.01, and one with C=10. You can use the `plot_coefficients` helper function (see above). Only plot the coefficients of the model that separates the character `tsu` from the other characters. You can assume that a one-vs-rest-like approach is used for multi-class problems, hence the n-th set of coefficients belong to the model that separates the n-th class. You should get three plots in total.\n",
    "\n",
    "First split the data in a default stratified train-test split. Train the models on the training data and score the accuracy on the test data. Add the C-value and accuracy to the title of the plots. \n",
    "\n",
    "Note: You may get convergence warnings. If so, just increase the number of optimization iterations (`max_iter`). Especially models with high C values can take longer to converge (can you guess why?). You can also choose to ignore these warnings since they won't affect the results much.  \n",
    "Note 2: Scikit-learn actually uses [a more sophisticated approach](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py) here than simple one-vs-all. It uses the fact that Logistic Regression predicts probabilities, and hence the probabilities of each class are taken into account (in a softmax function). It will still produce one model per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcTm2qCiNYyD"
   },
   "outputs": [],
   "source": [
    "# Implement. Do not change the name or signature of this function.\n",
    "def plot_tsu_coefficients(X,y):\n",
    "    \"\"\" Plots 28x28 heatmaps showing the coefficients of three Logistic \n",
    "    Regression models, each with different amounts of regularization values.\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    \n",
    "    Returns: 3 plots, as described above.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    C = [10**(-6), 10**(-2), 10**1]\n",
    "    for c in C:\n",
    "        model = LogisticRegression(C=c, max_iter=250))\n",
    "        scores_temp = cross_validate(model, X, y, cv=5, scoring='accuracy', return_train_score=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        plot_coefficients(model.coef_, \"C: \" + x)\n",
    "            \n",
    "    \"\"\" Renders a 28x28 heatmap of the model's trained coefficients.\n",
    "    Keyword arguments:\n",
    "    coef -- the model coefficients\n",
    "    name -- a title for the produced figure\n",
    "    \"\"\"\n",
    "    \n",
    "plot_tsu_coefficients(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28Ub828yg25q"
   },
   "source": [
    "## Question 2.2: Interpretation (1 points)\n",
    "Interpret the results. Which model works best? What is each of the models paying attention to when making predictions? Does that make sense - i.e. did the model learn something useful about the character *tsu*? Compare this to the results of question 1.2 and 1.3: does that help explain the results? Please formulate your answer in the string variable below. Keep your answer within 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcQPU_eyY7Si",
    "outputId": "530eebe3-9ec9-4a0b-f571-002f85fff069"
   },
   "outputs": [],
   "source": [
    "q_2_2 = \"\"\"\n",
    "        Your answer \n",
    "        \"\"\"\n",
    "\n",
    "if len(q_2_2.strip()) > 500:\n",
    "  print(\"Answer is {} characters long. Please shorten it to 500 characters.\".format(len(q_2_2.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrk9RwEeRl_Y"
   },
   "source": [
    "## Question 3.1: Mistake analysis (2 point)\n",
    "Let's focus more on the character 'tsu' and the behavior of the model with C=1e-6.\n",
    "\n",
    "First split the data again in a default stratified train-test split. Train the models on the training data and produce the predictions on the test data.\n",
    "\n",
    "Next, take the test examples which actually represent 'tsu'. From these, identify the ones which are predicted correctly and which ones are not.\n",
    "\n",
    "Finally, plot these examples using the `plot_examples` function, together with the predicted class (character). Create two plots (e.g. by calling `plot_examples` twice): one with 20 examples of 'tsu' characters which are predicted correctly, and a second with 20 examples of 'tsu' characters which are predicted incorrectly by this model. Indicate in the figure `title` which 'tsu' characters are correct and which ones are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_zYvqI-Ti4m"
   },
   "outputs": [],
   "source": [
    "# Implement. Do not change the name or signature of this function.\n",
    "def plot_mistakes(X,y):\n",
    "    \"\"\" Plots two sets of images. The first set shows 20 examples of characters\n",
    "    predicted correctly by a Logistic Regression classifier with C=1e-6. The \n",
    "    second set shows 20 examples of misclassifications. \n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    Returns: 2 sets of plots, as described above.\n",
    "    \"\"\"\n",
    "    pass\n",
    "plot_mistakes(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNs39QAvUTqw"
   },
   "source": [
    "## Question 3.2: Interpretation (1 point)\n",
    "Interpret the results. Can you explain which kinds of 'tsu' characters are predicted correctly and which ones are not? Compare this with what you observed in question 2.1 and 2.2. What does that tell you about the model? Please formulate your answer in the string variable below. Keep your answer within 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBBVz4AamVW6"
   },
   "outputs": [],
   "source": [
    "q_3_2 = \"\"\"\n",
    "        Your answer \n",
    "        \"\"\"\n",
    "\n",
    "if len(q_3_2.strip()) > 500:\n",
    "  print(\"Answer is {} characters long. Please shorten it to 500 characters.\".format(len(q_3_2.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzwZQ0OHtVxQ"
   },
   "source": [
    "## Question 4 (1 point)\n",
    "\n",
    "Archeologists found an ancient text in the mountain cave temples of Shodoshima, which is partially decifered into the following sentence:\n",
    "***\n",
    "**<word 1>** LOOKS BEAUTIFUL OVER THE **<word 2>**\n",
    "***\n",
    "\n",
    "They need your help to uncover the meaning of the missing characters and expand our knowledge on early Japanese literature. \n",
    "\n",
    "They sent us pictures of the missing characters in the form of 28x28 numpy arrays (because why not). This file is included. If not, the code below loads it into the notebook.\n",
    "\n",
    "Both words consist of two characters as shown below. The first two characters form the first word and the last two form the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e20B8UHQDnKD",
    "outputId": "b917e353-b33e-4aa1-8073-d692db5b2a99"
   },
   "outputs": [],
   "source": [
    "# Uncomment this code if you don't have the mystery_characters.npy file.\n",
    "# !pip install gdown\n",
    "# import gdown\n",
    "# url = 'https://drive.google.com/uc?id=1CcnG1a6feMd7n8rOph_nSBf29ltuJ9no'\n",
    "# output = 'mystery_characters.npy'\n",
    "# gdown.download(url, output, quiet=False)\n",
    "\n",
    "temple_data = np.load('mystery_characters.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "8i7lANHNIvFl",
    "outputId": "a43ee29a-172b-492c-f8e2-2bc86b1c9b44"
   },
   "outputs": [],
   "source": [
    "plot_examples(temple_data[0:2], None, row_length=2,title=\"Word 1\")\n",
    "plot_examples(temple_data[2:], None, row_length=2,title=\"Word 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKWBFr5dLlGg"
   },
   "source": [
    "Use your best model from question 1.2 to classify these images. Implement a small method `predict_characters` that prints the right classes (e.g. 'tsu')given an array of character. Once you have translated them into modern Japanese, translate them to English and type the two words in the variables `q_4_word_1` and `q_4_word_2`.\n",
    "\n",
    "Hint: You can use Google Translate if you don't know Japanese. Enter the words in Google Translate without spaces between the characters. There may be multiple meanings for a word, you can pick the one that fits the sentence best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xa9IstKgYnNy",
    "outputId": "b0d839c2-bc49-4d5a-fc2a-eedeb51b0218"
   },
   "outputs": [],
   "source": [
    "# Implement. Do not change the name or signature of this function.\n",
    "def predict_characters(X, y, X_test):\n",
    "    \"\"\" Print the class names for all the images in X.\n",
    "    X -- the data for training and testing\n",
    "    y -- the correct labels\n",
    "    X_test -- the new input images as 1D arrays\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "predict_characters(X, y, temple_data)\n",
    "\n",
    "# Fill in the correct meaning\n",
    "q_4_word_1 = \"??\"\n",
    "q_4_word_2 = \"??\"\n",
    "\n",
    "print(\"The sentence is : {} looks beautiful over the {}.\".format(q_4_word_1,q_4_word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgmIvbb09T6b"
   },
   "source": [
    "## Question 5: Bonus question\n",
    "\n",
    "### Question 5.1 : HOG features (1 point)\n",
    "Until now, we only used the pixel representation. There are may other ways of feature representation that work much better. One of these is the histogram of oriented gradients (HOG), which counts the occurrences of gradient orientations in localized portions of an image. You can read more about HOG feature descriptors [here](https://towardsdatascience.com/hog-histogram-of-oriented-gradients-67ecd887675f).\n",
    "\n",
    "Using the **scikit-image** implementation, compute the hog features of an image containing the *tsu* character. You can have a look at this [tutorial](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html). Implement a method `plot_hog_feature` showing an original image and its hog representation side by side. Play with different cell sizes to get a visually better representation. Finally, implement a method `plot_hog_features` that plots multiple cell sizes at once.\n",
    "\n",
    "This question is deliberative more free-from. You can make your own decisions as long as you adhere to the general method signatures and produce the right output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fkIzu981odq"
   },
   "outputs": [],
   "source": [
    "def plot_hog_feature(original_image, hog_image, cell_size = 2):\n",
    "  pass\n",
    "\n",
    "def plot_hog_features(X,y):\n",
    "  pass\n",
    "\n",
    "plot_hog_features(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkvJtbj09T6d"
   },
   "source": [
    "## Question 5.2 : A better model? (1 point)\n",
    "\n",
    "Compute the HOG features for all images in the dataset and train a Logistic Regression model based on the computed features. Explore different cell sizes, and different levels of regularization, to improve your evaluation accuracy while reducing the number of features for each image. Implement a method `compute_hog_feats` that computes all the HOG features, and a method `evaluate_hog_lr` that evaluates the resulting Logistic Regression model and prints out the test set accuracy. You can use a single holdout in this case. Is this model better than the one you found in question 1.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axELVBfo9T6e"
   },
   "outputs": [],
   "source": [
    "def compute_hog_feats(X_original, cell_size):\n",
    "  pass\n",
    "\n",
    "def evaluate_hog_lr(X,y):\n",
    "  pass\n",
    "\n",
    "# Don't forget to explore different cell sizes for HOG\n",
    "# compute_hog_feats should be called multiple times\n",
    "# Don't forget to tune the LogisticRegression hyperparameters. This can be done \n",
    "# offline, don't tune them inside evaluate_hog_lr."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1 - Model Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
